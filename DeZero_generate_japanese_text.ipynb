{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeZero_generate_japanese_text",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/DeZero_generate_japanese_text/blob/master/DeZero_generate_japanese_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU6uMMgamNUp",
        "colab_type": "text"
      },
      "source": [
        "フレームワーク **DeZero** のインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9d_aWXHcNhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install dezero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3i1Z2blmVtG",
        "colab_type": "text"
      },
      "source": [
        "形態素分析ライブラリー **janome** のインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFHkiEghA8qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install janome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa4uTXHmmljw",
        "colab_type": "text"
      },
      "source": [
        "日本語データセット **Nekoクラス**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu38qd0ya-Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import dezero\n",
        "from dezero.datasets import Dataset\n",
        "from dezero.utils import get_file, cache_dir\n",
        "import zipfile\n",
        "import re\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "class Neko(Dataset):\n",
        "    \n",
        "    def prepare(self):\n",
        "        url = 'https://www.aozora.gr.jp/cards/000148/files/789_ruby_5639.zip'\n",
        "        file = get_file(url)  \n",
        "        data = self.unzip(cache_dir + '/' + '789_ruby_5639.zip')  \n",
        "        self.text = self.preprocess(cache_dir + '/' + 'wagahaiwa_nekodearu.txt')\n",
        "        self.wakati = self.keitaiso(self.text)\n",
        "        self.corpus, self.word_to_id, self.id_to_word = self.process(self.wakati)\n",
        "        self.data = np.array(self.corpus[:-1])\n",
        "        self.label = np.array(self.corpus[1:])\n",
        "    \n",
        "    def unzip(self, file_path):\n",
        "        with zipfile.ZipFile(file_path) as existing_zip:\n",
        "            existing_zip.extractall(cache_dir)\n",
        "            \n",
        "    def preprocess(self, file_path):\n",
        "        binarydata = open(file_path, 'rb').read()\n",
        "        text = binarydata.decode('shift_jis')        \n",
        "                   \n",
        "        text = re.split(r'\\-{5,}', text)[2]  # ヘッダの削除\n",
        "        text = re.split('底本：',text)[0]   # フッタの削除\n",
        "        text = re.sub('｜', '', text)  # | の削除\n",
        "        text = re.sub('［.+?］', '', text)  # 入力注の削除\n",
        "        text = re.sub(r'《.+?》', '', text)  # ルビの削除\n",
        "        text = re.sub(r'\\u3000', '', text)  # 空白の削除\n",
        "        text = re.sub(r'\\r\\n', '', text)  # 改行の削除\n",
        "        text = text[1:]  # 先頭の１文字を削除（調整）\n",
        "        return text\n",
        " \n",
        "    def keitaiso(self, text):\n",
        "        t = Tokenizer()\n",
        "        output = t.tokenize(text, wakati=True)\n",
        "        return output\n",
        "     \n",
        "    def process(self, text):\n",
        "        # word_to_id, id_to_ward の作成\n",
        "        word_to_id, id_to_word = {}, {}\n",
        "        for word in text:\n",
        "            if word not in word_to_id:\n",
        "                new_id = len(word_to_id)\n",
        "                word_to_id[word] = new_id\n",
        "                id_to_word[new_id] = word\n",
        "\n",
        "        # corpus の作成\n",
        "        corpus = np.array([word_to_id[W] for W in text])\n",
        "        return corpus, word_to_id, id_to_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBYXnooWm1nT",
        "colab_type": "text"
      },
      "source": [
        "データセットの作成 ( **Nekoクラス**のインスタンス化。実行に数十秒ほど掛かります)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIq-wn8FBF_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neko = Neko()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLlULX5EIp6P",
        "colab_type": "text"
      },
      "source": [
        "作成したデータセットの内容を見てみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5D0D16TBkIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('neko.text = ', neko.text[:50])\n",
        "print('neko.wakati = ', neko.wakati[:15])\n",
        "print('neko.corpus = ',neko.corpus[:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfHWWMevQPDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('neko.word_to_id [猫] = ', neko.word_to_id['猫'])\n",
        "print('neko.id_to_word [6] = ', neko.id_to_word[6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y64F6lHqM3Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('neko.data = ', neko.data[:15])\n",
        "print('neko.label = ', neko.label[:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMghg_w3dPEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('length od data = ', len(neko.data))\n",
        "print('vaocab_size = ', len(neko.word_to_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDltLJ1AnFez",
        "colab_type": "text"
      },
      "source": [
        "**コード本体**です。１epoch毎に、100単語の文章を生成します。1eopch当たりの処理時間は2分くらいです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLY49G3RbdKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import dezero\n",
        "from dezero import Model\n",
        "from dezero import SeqDataLoader\n",
        "import dezero.functions as F\n",
        "import dezero.layers as L\n",
        "import random\n",
        "from dezero import cuda \n",
        "import textwrap\n",
        "\n",
        "max_epoch = 70\n",
        "batch_size = 30 \n",
        "vocab_size = len(neko.word_to_id)  \n",
        "wordvec_size = 650  \n",
        "hidden_size = 650\n",
        "bptt_length = 30  \n",
        "\n",
        "class Lstm_nlp(Model):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size, out_size):\n",
        "        super().__init__()\n",
        "        self.embed = L.EmbedID(vocab_size, wordvec_size)\n",
        "        self.rnn = L.LSTM(hidden_size)\n",
        "        self.fc = L.Linear(out_size)\n",
        "\n",
        "    def reset_state(self):  # 状態リセット\n",
        "        self.rnn.reset_state()\n",
        "\n",
        "    def __call__(self, x):  # レイヤの接続内容を記載\n",
        "        y = self.embed(x) \n",
        "        y = self.rnn(y)\n",
        "        y = self.fc(y)\n",
        "        return y\n",
        "\n",
        "model = Lstm_nlp(vocab_size, wordvec_size, hidden_size, vocab_size)  # モデル生成\n",
        "dataloader = SeqDataLoader(neko, batch_size=batch_size)  # データローダ生成\n",
        "seqlen = len(neko)\n",
        "optimizer = dezero.optimizers.Adam().setup(model)  # 最適化手法は Adam\n",
        "\n",
        "# GPUの有無判定と処理\n",
        "if dezero.cuda.gpu_enable:  # GPUが有効であれば下記を実行\n",
        "    dataloader.to_gpu()  # データローダをGPUへ\n",
        "    model.to_gpu()  # モデルをGPUへ\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(max_epoch):\n",
        "    model.reset_state()\n",
        "    loss, count = 0, 0\n",
        "\n",
        "    for x, t in dataloader:\n",
        "        y = model(x)  # 順伝播\n",
        "\n",
        "        # y は次の単語の出現度合いを表すベクトル(vocab_size次元)。\n",
        "        # y にsoftmaxを掛け出現確率にしたものとワンホットの次の正解データからロス計算。\n",
        "        # 但し、入力 t はワンホットベクトルの何番目に1が立っているかを表す数字(整数)。\n",
        "        loss += F.softmax_cross_entropy_simple(y, t)  \n",
        "        count += 1\n",
        "\n",
        "        if count % bptt_length == 0 or count == seqlen:\n",
        "            model.cleargrads()  # 微分の初期化\n",
        "            loss.backward()  # 逆伝播\n",
        "            loss.unchain_backward()  # 計算グラフを遡ってつながりを切る\n",
        "            optimizer.update()  # 重みの更新\n",
        "    avg_loss = float(loss.data) / count\n",
        "    print('| epoch %d | loss %f' % (epoch + 1, avg_loss))\n",
        "\n",
        "    # 文章生成\n",
        "    model.reset_state()  # 状態をリセット\n",
        "    with dezero.no_grad():  # 重みの更新をしない\n",
        "         text = []\n",
        "         x = random.randint(0,vocab_size)  # 最初の単語番号をランダムに選ぶ\n",
        "         while len(text)  < 100:  # 100単語になるまで繰り返す\n",
        "               x = np.array(int(x))\n",
        "               y = model(x)  # yは次の単語の出現度合い\n",
        "               p = F.softmax_simple(y, axis=0)  # softmax を掛けて出現確率にする\n",
        "               xp = cuda.get_array_module(p)  # GPUがあれば xp=cp なければ xp=np\n",
        "               sampled = xp.random.choice(len(p.data), size=1, p=p.data)  #　出現確率を考慮して数字(インデックス)を選ぶ\n",
        "               word = neko.id_to_word[int(sampled)]  # 数字を単語に変換\n",
        "               text.append(word)  # text に単語を追加\n",
        "               x = sampled  # sampledを次の入力にする\n",
        "         text = ''.join(text)\n",
        "         print(textwrap.fill(text, 60))  # 60文字で改行して表示\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}